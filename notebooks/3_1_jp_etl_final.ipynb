{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3RXh6CFp4v9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l0d8DZneUBV",
        "outputId": "d29fe474-1d58-4625-a634-0a36bab4b1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Se conecta Google Colaboratory con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instala pyspark en Google Colaboratory\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_EC4DAdBKtq",
        "outputId": "fc2312c7-c3bc-44ad-b6b0-6e458961b443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpcWhUAdOkPV",
        "outputId": "59ccf801-8ee5-4dc4-d7ed-26e94ca2fd2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se importan las librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import json\n",
        "import pickle\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json, explode, sum,expr, array_contains, split, substring, concat_ws, lit, udf, collect_list, from_unixtime, year, month, dayofmonth\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from pyspark.conf import SparkConf\n"
      ],
      "metadata": {
        "id": "kchzfryFffl_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aumenta la memoria de la sesión de spark\n",
        "configuracion = SparkConf().set(\"spark.executor.memory\", \"8g\").set(\"spark.driver.memory\", \"8g\")\n",
        "spark = SparkSession.builder.appName(\"Dataframes\").config(conf=configuracion).getOrCreate()"
      ],
      "metadata": {
        "id": "K1JKSy-ZCTbF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se cargan los archivos de la carpeta metadata-sitios y se compilan en un solo DataFrame\n",
        "\n",
        "metadata_sitios=[]\n",
        "\n",
        "for i in range(1,12):\n",
        "\n",
        "\n",
        "    archivo = spark.read.json(f\"/content/drive/My Drive/Google Maps/metadata-sitios/{i}.json\")\n",
        "\n",
        "    archivo = archivo.withColumn(\"MISC\", col(\"MISC\").cast(\"string\"))\n",
        "\n",
        "    metadata_sitios.append(archivo)\n",
        "\n",
        "df_final = metadata_sitios[0]\n",
        "\n",
        "for dataframe in metadata_sitios[1:]:\n",
        "\n",
        "  df_final = df_final.unionByName(dataframe)\n",
        "\n",
        "\n",
        "metadata_sitios=df_final"
      ],
      "metadata": {
        "id": "3Nqg6JYzC4IT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de columnas a eliminar\n",
        "columnas_a_eliminar = ['MISC', 'state', 'price', 'hours', 'description','url']\n",
        "\n",
        "# Elimina las columnas especificadas\n",
        "metadata_sitios = metadata_sitios.select([columna for columna in metadata_sitios.columns if columna not in columnas_a_eliminar])\n",
        "\n",
        "#Pasa la columna 'category' a tipo string\n",
        "metadata_sitios = metadata_sitios.withColumn(\"category\", col(\"category\").cast(\"string\"))\n",
        "\n",
        "# Filtra las filas que contienen 'restaurant' en la columna 'category'\n",
        "metadata_sitios = metadata_sitios.filter((col(\"category\").contains('restaurant')) | (col(\"category\").contains('Restaurant')))\n",
        "\n",
        "# Utiliza la función substring para eliminar el primer y último caracter de la columna 'category'\n",
        "metadata_sitios = metadata_sitios.withColumn(\"category\", expr(\"substring(category, 2, length(category)-2)\"))\n",
        "\n",
        "# Separa la columna 'address' en 3 columnas: 'address', 'city' y 'state'\n",
        "\n",
        "split_col = split(metadata_sitios['address'], ',')\n",
        "metadata_sitios = metadata_sitios.withColumn('state', substring(split_col.getItem(3), 2, 2))\n",
        "metadata_sitios = metadata_sitios.withColumn('city', split_col.getItem(2))\n",
        "metadata_sitios = metadata_sitios.withColumn('address', concat_ws(', ', split_col.getItem(0), split_col.getItem(1)))\n",
        "\n",
        "# Filtrar las filas con estados 'FL', 'VA' o 'NV'\n",
        "estados_seleccionados = ['FL', 'VA', 'NV']\n",
        "metadata_sitios = metadata_sitios.filter(metadata_sitios['state'].isin(estados_seleccionados))\n",
        "\n"
      ],
      "metadata": {
        "id": "sOWlCYXHSQ81"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se leen los reviews de Nevada\n",
        "\n",
        "reviews_Nevada=[]\n",
        "\n",
        "\n",
        "bandera=True\n",
        "\n",
        "i=1\n",
        "\n",
        "while bandera:\n",
        "\n",
        "    try:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        archivo = spark.read.json(f\"/content/drive/My Drive/Google Maps/reviews-estados/review-Nevada/{i}.json\")\n",
        "\n",
        "\n",
        "\n",
        "        reviews_Nevada.append(archivo)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    except:\n",
        "\n",
        "        bandera=False\n",
        "\n",
        "\n",
        "df_final = reviews_Nevada[0]\n",
        "\n",
        "for dataframe in reviews_Nevada[1:]:\n",
        "\n",
        "  df_final = df_final.unionByName(dataframe)\n",
        "\n",
        "\n",
        "reviews_Nevada=df_final\n",
        "\n"
      ],
      "metadata": {
        "id": "61ZwaPgQoYg4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se leen los reviews de Florida\n",
        "\n",
        "reviews_Florida=[]\n",
        "\n",
        "\n",
        "bandera=True\n",
        "\n",
        "i=1\n",
        "\n",
        "while bandera:\n",
        "\n",
        "    try:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        archivo = spark.read.json(f\"/content/drive/My Drive/Google Maps/reviews-estados/review-Florida/{i}.json\")\n",
        "\n",
        "\n",
        "\n",
        "        reviews_Florida.append(archivo)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    except:\n",
        "\n",
        "        bandera=False\n",
        "\n",
        "\n",
        "df_final = reviews_Florida[0]\n",
        "\n",
        "for dataframe in reviews_Florida[1:]:\n",
        "\n",
        "  df_final = df_final.unionByName(dataframe)\n",
        "\n",
        "\n",
        "reviews_Florida=df_final"
      ],
      "metadata": {
        "id": "p8DZvx8P25KC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se leen los reviews de Virginia\n",
        "\n",
        "reviews_Virginia=[]\n",
        "\n",
        "\n",
        "bandera=True\n",
        "\n",
        "i=1\n",
        "\n",
        "while bandera:\n",
        "\n",
        "    try:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        archivo = spark.read.json(f\"/content/drive/My Drive/Google Maps/reviews-estados/review-Virginia/{i}.json\")\n",
        "\n",
        "\n",
        "\n",
        "        reviews_Virginia.append(archivo)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "\n",
        "    except:\n",
        "\n",
        "        bandera=False\n",
        "\n",
        "\n",
        "df_final = reviews_Virginia[0]\n",
        "\n",
        "for dataframe in reviews_Virginia[1:]:\n",
        "\n",
        "  df_final = df_final.unionByName(dataframe)\n",
        "\n",
        "\n",
        "reviews_Virginia=df_final"
      ],
      "metadata": {
        "id": "fYJDPZEH3J2f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar la columna 'state' con valores constantes a cada DataFrame\n",
        "reviews_Nevada = reviews_Nevada.withColumn('state', lit('NV'))\n",
        "reviews_Florida = reviews_Florida.withColumn('state', lit('FL'))\n",
        "reviews_Virginia = reviews_Virginia.withColumn('state', lit('VA'))\n",
        "\n",
        "# Concatenar los DataFrames\n",
        "reviews_google = reviews_Nevada.union(reviews_Florida).union(reviews_Virginia)\n",
        "\n",
        "#Transformo la columna 'time' desde el formato Unix\n",
        "\n",
        "reviews_google = reviews_google.withColumn(\"time\", from_unixtime(col(\"time\") / 1000).cast(\"timestamp\"))\n",
        "\n",
        "# Crea columnas 'year', 'month' y 'day'\n",
        "reviews_google = reviews_google.withColumn(\"year\", year(\"time\"))\n",
        "reviews_google = reviews_google.withColumn(\"month\", month(\"time\"))\n",
        "reviews_google = reviews_google.withColumn(\"day\", dayofmonth(\"time\"))\n",
        "\n",
        "# Elimina la columna 'time'\n",
        "reviews_google = reviews_google.drop(\"time\")\n",
        "\n",
        "# Lista de columnas a eliminar\n",
        "columnas_a_eliminar = ['pics', 'resp']\n",
        "\n",
        "# Elimina las columnas especificadas\n",
        "reviews_google = reviews_google.select([columna for columna in reviews_google.columns if columna not in columnas_a_eliminar])\n",
        "\n",
        "# Obtener una lista de gmap_id únicos\n",
        "gmap_id_unicos = metadata_sitios.select(\"gmap_id\").distinct().collect()\n",
        "\n",
        "# Extraer los valores únicos de la lista\n",
        "gmap_id_unicos = [row[\"gmap_id\"] for row in gmap_id_unicos]\n",
        "\n",
        "#Toma las columnas que pertenecian originalmente a reviews_google\n",
        "\n",
        "reviews_google = reviews_google.filter(col(\"gmap_id\").isin(gmap_id_unicos))\n",
        "\n",
        "\n",
        "\n",
        "# Inicializa el analizador de sentimientos Vader\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define una función UDF para aplicar el análisis de sentimientos y asignar los valores 0, 1 o 2\n",
        "def vader_sentiment_analysis(text,rating):\n",
        "     # Verifica si el texto no es None o vacío\n",
        "    if text and isinstance(text, str):\n",
        "        compound_score = analyzer.polarity_scores(text)['compound']\n",
        "        if compound_score >= 0.05:\n",
        "            return 2  # Positivo\n",
        "        elif compound_score > -0.05 and compound_score < 0.05:\n",
        "            return 1  # Neutral\n",
        "        else:\n",
        "            return 0  # Negativo\n",
        "    else:\n",
        "        # Verifica el valor de la columna 'rating'\n",
        "        if rating >= 4.0:\n",
        "            return 2\n",
        "        elif rating < 3.0:\n",
        "            return 0\n",
        "        else:\n",
        "\n",
        "            return 1\n",
        "\n",
        "# Registrar la función UDF\n",
        "vader_sentiment_analysis_udf = udf(vader_sentiment_analysis, StringType())\n",
        "\n",
        "reviews_google = reviews_google.withColumn('sentiment', vader_sentiment_analysis_udf(reviews_google['text'], reviews_google['rating']))\n",
        "\n"
      ],
      "metadata": {
        "id": "lHYVgFf01DOS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elimina duplicados y seleccionar las columnas 'user_id' y 'name'\n",
        "user_google = reviews_google.select('user_id', 'name').dropDuplicates()\n",
        "\n",
        "# Elimina la columna 'name' del DataFrame original en PySpark\n",
        "reviews_google = reviews_google.drop('name')"
      ],
      "metadata": {
        "id": "B60haO_L5Ls2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se lee el archivo business de yelp\n",
        "\n",
        "with open(\"/content/drive/My Drive/Yelp/business.pkl\", 'rb') as archivo:\n",
        "\n",
        "        business_yelp = pickle.load(archivo)\n"
      ],
      "metadata": {
        "id": "tbwLNLDZoeXB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina las columnas repetidas que tienen casi todos sus datos nulos\n",
        "business_yelp=business_yelp.iloc[:,0:14]\n",
        "\n",
        "# Lista de columnas a eliminar\n",
        "columnas_a_eliminar = ['hours', 'attributes', 'is_open']\n",
        "\n",
        "# Elimina las columnas especificadas\n",
        "business_yelp = business_yelp.drop(columnas_a_eliminar, axis=1)\n",
        "\n",
        "# Cambiar el nombre de la columna 'review_count' a 'num_of_reviews'\n",
        "business_yelp= business_yelp.rename(columns={'review_count': 'num_of_reviews'})\n",
        "\n",
        "# Filtra las filas donde la columna 'categories' contiene la cadena 'Restaurants'\n",
        "business_yelp = business_yelp[business_yelp['categories'].str.contains('Restaurants', case=False, na=False)]\n",
        "\n",
        "#Filtra los datos de los tres estados que necesitamos\n",
        "business_yelp=business_yelp[(business_yelp['state']=='NV') | ((business_yelp['state']=='FL') | (business_yelp['state']=='VA'))]\n",
        "\n",
        "business_yelp.reset_index(inplace=True,drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "B6G5WEYCVOVO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se lee el archivo review de yelp\n",
        "\n",
        "review_yelp = spark.read.json(\"/content/drive/My Drive/Yelp/review.json\")"
      ],
      "metadata": {
        "id": "yBgZYKP84iLl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "business_ids= business_yelp['business_id'].tolist()\n",
        "\n",
        "#Cambia el nombre de la columna 'stars' a 'rating'\n",
        "\n",
        "review_yelp = review_yelp.withColumnRenamed(\"stars\", \"rating\")\n",
        "\n",
        "#Hace analisis de sentimientos\n",
        "\n",
        "review_yelp = review_yelp.withColumn('sentiment', vader_sentiment_analysis_udf(review_yelp['text'],review_yelp['rating']))\n",
        "\n",
        "# Filtrar las filas en review_yelp donde 'business_id' está en la lista\n",
        "review_yelp = review_yelp.filter(col('business_id').isin(business_ids))\n",
        "\n",
        "# Convierte la columna 'date' a tipo timestamp\n",
        "review_yelp = review_yelp.withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
        "\n",
        "# Crear las columnas 'year', 'month' y 'day'\n",
        "review_yelp = review_yelp.withColumn(\"year\", year(\"date\"))\n",
        "review_yelp = review_yelp.withColumn(\"month\", month(\"date\"))\n",
        "review_yelp = review_yelp.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "\n",
        "# Eliminar la columna original 'date'\n",
        "review_yelp = review_yelp.drop(\"date\")\n",
        "\n",
        "#Toma las columnas que necesito\n",
        "\n",
        "reviews_yelp=user_google = review_yelp.select('review_id','business_id','user_id', 'rating','year','month','day','text','sentiment')"
      ],
      "metadata": {
        "id": "JI3tpm55YmkP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se lee el archivo tip de yelp\n",
        "\n",
        "tip_yelp=[]\n",
        "\n",
        "with open(\"/content/drive/My Drive/Yelp/tip.json\",\"r\", encoding= 'utf-8') as filejson:\n",
        "\n",
        "\n",
        "                # Procesa cada línea del archivo como un objeto JSON\n",
        "                for linea in filejson:\n",
        "\n",
        "                        # Intenta cargar la línea como un objeto JSON\n",
        "                        objeto = json.loads(linea)\n",
        "\n",
        "                        tip_yelp.append(objeto)\n",
        "\n",
        "tip_yelp=pd.DataFrame(tip_yelp)\n"
      ],
      "metadata": {
        "id": "HjKmJdRW4a7D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar las filas basadas en 'business_ids'\n",
        "tip_yelp= tip_yelp[tip_yelp['business_id'].isin(business_ids)]\n",
        "\n",
        "# Convertir la columna 'date' a tipo datetime\n",
        "tip_yelp['date'] = pd.to_datetime(tip_yelp['date'])\n",
        "\n",
        "# Crear las columnas 'year', 'month' y 'day'\n",
        "tip_yelp['year'] = tip_yelp['date'].dt.year\n",
        "tip_yelp['month'] = tip_yelp['date'].dt.month\n",
        "tip_yelp['day'] = tip_yelp['date'].dt.day\n",
        "\n",
        "# Eliminar la columna original 'date'\n",
        "tip_yelp = tip_yelp.drop(columns=['date','compliment_count'])\n",
        "\n",
        "tip_yelp.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "65T9kHsC7X42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003af8b4-c016-4dbf-b768-fd57bfb92c90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-7c70b2105ad0>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  tip_yelp['date'] = pd.to_datetime(tip_yelp['date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se lee el archivo user de yelp\n",
        "\n",
        "users_yelp= pd.read_parquet(\"/content/drive/My Drive/Yelp/user.parquet\")"
      ],
      "metadata": {
        "id": "1GXEH84p4so7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona las columnas 'user_id' y 'name'\n",
        "users_yelp = users_yelp[['user_id', 'name']]"
      ],
      "metadata": {
        "id": "cOVk7Dr92iBl"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}